# 作業題目與解答

## 目錄
1. [創意題](#創意題)
2. [估算與探索資料能力](#估算與探索資料能力)
3. [思考題](#思考題)
4. [估算題](#估算題)
5. [成果題](#成果題)
6. [計算題](#計算題)

---

## 創意題

### **題目：**
今天你站在南京復興路口，如何透過一個人的力量估算尖峰時間 10 分鐘內的車流量？

### **解答：**
為了在南京復興路口以一人之力估算尖峰時間10分鐘內的車流量，可以採取以下步驟：

1. **選定觀測位置：**
   - 找到一個視野良好且安全的位置，能夠清楚觀察到路口所有方向的車輛。

2. **錄製車流影片：**
   - 使用手機或攝影設備錄製多段短影片，捕捉不同時間段的車流情況。
   - **錄製計劃：**
     - **時長：** 分別錄製 10 秒、30 秒和 60 秒的影片。
     - **次數：** 每個時長的影片各錄製 5 次，確保數據的可靠性。

3. **數據統計與計算：**
   - 回放並統計每段影片中經過的車輛數量。
   - **計算平均值：**
     - 對每個時長的多次測量結果取平均值。
   - **推算10分鐘車流量：**
     - **10 秒影片：** 平均值 × 60 = 10 分鐘車流量
     - **30 秒影片：** 平均值 × 20 = 10 分鐘車流量
     - **60 秒影片：** 平均值 × 10 = 10 分鐘車流量
   - **最終估算：**
     - 將以上三種方法得到的結果取平均值，作為最終的10分鐘車流量估算。

4. **誤差校正：**
   - 考慮特殊情況（如紅綠燈變化、突發事件）對數據進行適當調整。
   - 若有可能，與官方數據進行比對，評估估算的準確性。

### **優點：**
- **高效性：** 利用短時間的觀測即可推算出較準確的數據。
- **可靠性：** 多次取樣和平均值計算降低了偶然因素帶來的誤差。
- **可行性：** 僅需一人和簡單設備即可完成，操作簡便。

---

## 估算與探索資料能力

### **題目：**
若今日 YouBike 廠商提供 2024 年 07 月的各站點的騎乘資料給我們，我們怎麼驗證對方提供的資料可能有漏？

### **解答：**
為了驗證 YouBike 廠商提供的 2024 年 07 月騎乘資料是否完整，可採取以下方法：

1. **站點完整性檢查：**
   - **資料對比：**
     - 將**騎乘資料**與**站點資料**進行關聯（Join）操作。
   - **缺失值檢測：**
     - 檢查關聯後的資料集是否存在 `NULL` 或缺失值。
     - 若某些站點在騎乘資料中完全沒有記錄，需進一步確認該站點是否正常運營，否則可能存在資料遺漏。
   - **站點數量比對：**
     - 比較提供的站點數量與官方公佈的站點總數，確保無遺漏。

2. **時間連續性檢查：**
   - **每日資料檢查：**
     - 檢視7月份每日的騎乘記錄數量，確保每天都有合理數量的數據。
   - **異常值偵測：**
     - 若某天的騎乘次數異常偏低或為零，需確認是否有資料缺失或系統故障。
   - **趨勢分析：**
     - 繪製每日騎乘量的折線圖，觀察數據趨勢是否連續和平滑，突變點可能暗示資料缺漏。

---

## 思考題

### **題目：**
為何 Google 從海量資料根據使用者關鍵字提供搜尋結果可在幾秒鐘內完成，而不是幾分鐘也不是幾小時，可能關鍵因素是什麼？(論述不得超過 400 字)

### **解答：**
Google 能夠在短短幾秒鐘內從海量資料中提供相關的搜尋結果，主要歸功於以下關鍵因素：

1. **高效的索引技術：**
   - **倒排索引（Inverted Index）：**
     - Google 在爬取網頁後，會對內容建立倒排索引，將關鍵字對應到包含該詞的所有網頁文檔，類似於書籍的索引頁。
     - 這種結構使得在搜尋時，可以直接根據關鍵字快速定位相關的網頁，而不需逐一檢索所有文檔。
   - **持續更新與優化：**
     - 索引會持續更新，確保最新的網頁內容被收錄，同時利用壓縮和優化技術，減少索引的大小和查詢時間。

2. **分散式計算與儲存系統：**
   - **全球分佈的資料中心：**
     - Google 在全球各地建立了大量的資料中心，透過分散式系統將資料和計算任務分配到不同的伺服器，同時處理大量的搜尋請求。
   - **負載均衡與高可用性：**
     - 透過負載均衡技術，將請求均勻分配到各個伺服器，確保系統在高並發情況下仍能快速響應。

3. **先進的快取與預測技術：**
   - **結果快取：**
     - 對於高頻率的搜尋請求，Google 會將結果進行快取，直接返回預先準備好的結果，大幅降低響應時間。
   - **預先載入與預測：**
     - 利用使用者行為分析，預測可能的搜尋需求，提前準備相關資料，縮短實時查詢的處理時間。

---

## 估算題

### **題目：**
有 100 萬則 Facebook 粉絲頁平均 250 字左右中文字貼文資料，不包含留言，但包含貼文心情互動統計資料、發文時間資料、粉絲頁名稱、粉絲頁 ID 等欄位。請回答以下問題：

#### 1. 若要提供利用關鍵字進行全文搜尋這些貼文資料的需求，你會選擇哪一種的資料庫？

#### 2. 如果建立粉絲頁 ID 的索引，會需要多少記憶體空間？

#### 3. 如果建立貼文文字的中文索引，又會額外需要多少記憶體空間？

---

### **解答：**

#### 1. **選擇適合的資料庫：**
   - **推薦使用：** `Elasticsearch`
     - **理由：**
       - **全文檢索能力強大：** Elasticsearch 專為全文搜索和分析設計，能夠快速、高效地處理大量文本資料。
       - **分散式架構：** 能夠水平擴展，處理大量數據和高併發查詢。
       - **豐富的查詢功能：** 支援複雜的查詢和過濾條件，滿足多樣化的搜索需求。
       - **實時性：** 資料可以被即時索引和搜索，適合需要快速響應的應用場景。

#### 2. **粉絲頁 ID 索引記憶體空間估算：**
   - **假設：**
     - 每個粉絲頁 ID 佔用 `8 bytes`（例如，64位整數）。
     - 總共 `1,000,000` 條記錄。
   - **計算：**
     \[
     8\ bytes/record \times 1,000,000\ records = 8,000,000\ bytes
     \]
   - **結果：**
     - **約為：** `8 MB` 的記憶體空間。

#### 3. **貼文文字中文索引記憶體空間估算：**
   - **假設：**
     - 每則貼文平均 `250` 個中文字。
     - 每個中文字在 UTF-8 編碼下佔用 `3 bytes`。
     - 考慮到索引會對文本進行分詞，並儲存詞彙和其在文檔中的位置等資訊，索引大小通常會比原始文本大。
     - **估計索引放大率：** 約為原始文本大小的 `1.5 倍`（實際值視分詞和索引設定而定）。
   - **計算：**
     - **原始文本大小：**
       \[
       250\ characters/post \times 3\ bytes/character \times 1,000,000\ posts = 750,000,000\ bytes
       \]
     - **索引大小：**
       \[
       750,000,000\ bytes \times 1.5 = 1,125,000,000\ bytes
       \]
   - **結果：**
     - **約為：** `1.125 GB` 的記憶體空間。


---

## 成果題

### **題目：**
根據 [政府電子採購網資料視覺化平台](https://ronnywang.github.io/pcc-viewer/index.html) 萃取「資料分析」有關的標案列舉出來，並統計 2022 年至 2024 年間 Top 10 廠商得標金額與案數。請你分享你設定哪些關鍵字跟「資料分析有關」？為何這樣設定？

---

### **解答：**

#### **關鍵字設定：**
為了全面且準確地萃取與「資料分析」相關的標案，選定以下關鍵字：

1. **「資料分析」**
   - **理由：**
     - 直接且明確，涵蓋一般的數據分析與處理相關標案。

2. **「大數據」**
   - **理由：**
     - 反映現代資料處理的大規模和複雜性，相關標案可能涉及資料分析平台建設與應用。

3. **「資料探勘」**
   - **理由：**
     - 涵蓋從大量資料中挖掘有價值資訊的專業領域，與資料分析緊密相關。

4. **「人工智慧」**
   - **理由：**
     - 許多資料分析工作涉及 AI 技術，如機器學習和深度學習，相關標案可能涉及智能分析系統的開發。


#### **資料處理與統計注意事項：**

- **重複標案處理：**
  - **問題：**
    - 若出現 **`title` 相同但 **`job_number`** 不同** 的情況，需確認是否為同一標案的不同階段或部分。
  - **處理方式：**
    - **若屬同一標案：**
      - 可以根據 `title` 進行去重，並取最高的得標金額作為統計依據。
    - **若屬不同標案：**
      - 則分別計算各自的得標金額與案數，確保統計結果的準確性。

- **時間範圍篩選：**
  - 僅篩選 **2022 年至 2024 年** 之間的標案，確保數據符合題目要求。

- **Top 10 廠商統計：**
  - **統計項目：**
    - **得標金額總和：** 每個廠商在指定期間內的總得標金額。
    - **得標案數：** 每個廠商在指定期間內中標的總案數。
  - **排序方式：**
    - 根據 **得標金額總和** 進行降序排序，取前 10 名廠商。
  - **結果呈現：**
    - 建議使用 **表格** 形式展示，包含「廠商名稱」、「得標金額總和」、「得標案數」等欄位，清晰明瞭。

#### **結果呈現範例：**

結果在top_10_budget.csv中

| 排名 | 廠商名稱     | 得標金額總和 (新台幣) | 得標案數 |
|:----:|:------------:|---------------------:|---------:|
| 1    | XXX 科技股份有限公司 |     100,000,000     |       5 |
| 2    | YYY 資訊有限公司   |      85,500,000     |       3 |
| ...  | ...          |               ...     |     ... |

---

## 計算題

### **題目：**
自己 random 產生 2,000 萬個 float 並寫入到一個 txt 檔，一列一筆資料，最後實作一方法(方法不限)，使得這 2000 萬個 float 可在 30 秒內排序完成(小到大)。

---

### **解答：**
為了在30秒內完成對2000萬個浮點數的排序，可採用高效的數據生成和排序方法，以下是詳細步驟：

### **步驟 1：數據生成與寫入**

- **使用工具：**
  - `NumPy`：高效的科學計算庫，擅長大規模數據處理。
- **實作步驟：**
  1. **生成隨機浮點數：**
     - 使用 `numpy.random.rand()` 生成 2000 萬個 0 到 1 之間的隨機浮點數。
  2. **寫入文本檔案：**
     - 將生成的浮點數以每行一個的格式寫入到 `random_floats.txt` 文件中。
  3. **利用內建sort方法:**
     - Python內建排序方法為Timsort，已經是一種高效能排序方法，時間複雜度為O(nlogn)
- **程式碼實作：**
  ```python
  python3 test6.py
